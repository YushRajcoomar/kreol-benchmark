{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build the model off of HuggingFace libraries instead of fairseq scripts. It is easier to debug and understand what you're doing when building the model.\n",
    "We are starting from absolute scratch. We need to train a sentencepiece tokenizer on joint corpus. That's kinda done in the previous notebook.\n",
    "\n",
    "However I never got a hf model to train with the tokenizer. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers.implementations import SentencePieceBPETokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Model from hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_SIZE = 256  # Batch-size to train the tokenizer on\n",
    "TOKENIZER_VOCABULARY = 25000  # Total number of unique subwords the tokenizer can have\n",
    "\n",
    "BLOCK_SIZE = 128  # Maximum number of tokens in an input sample\n",
    "NSP_PROB = 0.50  # Probability that the next sentence is the actual next sentence in NSP\n",
    "SHORT_SEQ_PROB = 0.1  # Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding\n",
    "\n",
    "MLM_PROB = 0.2  # Probability with which tokens are masked in MLM\n",
    "\n",
    "TRAIN_BATCH_SIZE = 2  # Batch-size for pretraining the model on\n",
    "MAX_EPOCHS = 1  # Maximum number of epochs to train the model for\n",
    "LEARNING_RATE = 1e-4  # Learning rate for training the model\n",
    "\n",
    "MODEL_CHECKPOINT = \"mbart-large-50\"  # Name of pretrained model from ðŸ¤— Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding all the data i have in 1 large text file to train my tokenizer\n",
    "\n",
    "# paths = [str(x) for x in Path(\"../experiments/data/\").glob(\"**/*.jsonl\")]\n",
    "# paths_df = [pd.read_json(x,lines=True) for x in paths]\n",
    "\n",
    "# all_str = ''\n",
    "# for i in paths_df:\n",
    "#     add_str = ''.join(i['input'].values + i['target'].values)\n",
    "#     all_str += add_str\n",
    "\n",
    "# sentences = all_str.split(\". \")\n",
    "\n",
    "# with open(\"all_sentences.txt\", \"w\") as file:\n",
    "#     for sentence in sentences:\n",
    "#         file.write(sentence + \".\\n\")\n",
    "\n",
    "# !spm_train --input=\"/mnt/disk/yrajcoomar/kreol-benchmark/pipelines/all_sentences.txt\" --model_prefix=kreol --vocab_size=20000 --model_type=bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train tokenizer\n",
    "\n",
    "# def batch_iterator():\n",
    "#     for i in range(0, len(all_str), TOKENIZER_BATCH_SIZE):\n",
    "#         yield all_str[i : i + TOKENIZER_BATCH_SIZE]\n",
    "\n",
    "# tokenizer = SentencePieceBPETokenizer()\n",
    "# tokenizer.train_from_iterator(batch_iterator(), vocab_size=TOKENIZER_BATCH_SIZE, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "# ])\n",
    "\n",
    "\n",
    "# !mkdir tok\n",
    "# tokenizer.save_model(\"./tok/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## idrk whats going on\n",
    "\n",
    "# from tokenizers.implementations import SentencePieceBPETokenizer\n",
    "# from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "# tokenizer = SentencePieceBPETokenizer(\n",
    "#     \"tokenizer/vocab.json\",\n",
    "#     \"tokenizer/merges.txt\",\n",
    "# )\n",
    "# tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#     (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#     (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "# )\n",
    "# tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBart50Tokenizer\n",
    "\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"/mnt/disk/yrajcoomar/kreol-benchmark/pipelines/tok\",max_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20004, 35, 19939, 19929, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"kifr\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_XX'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([20004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_XX kifr</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(\"kifr\")['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.bos_token = \"<bos>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBart50Tokenizer(name_or_path='/teamspace/studios/this_studio/kreol-benchmark/pipelines/tok/', vocab_size=20054, model_max_length=256, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['ar_AR', 'cs_CZ', 'de_DE', 'en_XX', 'es_XX', 'et_EE', 'fi_FI', 'fr_XX', 'gu_IN', 'hi_IN', 'it_IT', 'ja_XX', 'kk_KZ', 'ko_KR', 'lt_LT', 'lv_LV', 'my_MM', 'ne_NP', 'nl_XX', 'ro_RO', 'ru_RU', 'si_LK', 'tr_TR', 'vi_VN', 'zh_CN', 'af_ZA', 'az_AZ', 'bn_IN', 'fa_IR', 'he_IL', 'hr_HR', 'id_ID', 'ka_GE', 'km_KH', 'mk_MK', 'ml_IN', 'mn_MN', 'mr_IN', 'pl_PL', 'ps_AF', 'pt_XX', 'sv_SE', 'sw_KE', 'ta_IN', 'te_IN', 'th_TH', 'tl_XX', 'uk_UA', 'ur_PK', 'xh_ZA', 'gl_ES', 'sl_SI']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20001: AddedToken(\"ar_AR\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20002: AddedToken(\"cs_CZ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20003: AddedToken(\"de_DE\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20004: AddedToken(\"en_XX\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20005: AddedToken(\"es_XX\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20006: AddedToken(\"et_EE\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20007: AddedToken(\"fi_FI\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20008: AddedToken(\"fr_XX\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20009: AddedToken(\"gu_IN\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20010: AddedToken(\"hi_IN\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20011: AddedToken(\"it_IT\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20012: AddedToken(\"ja_XX\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20013: AddedToken(\"kk_KZ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20014: AddedToken(\"ko_KR\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20015: AddedToken(\"lt_LT\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20016: AddedToken(\"lv_LV\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20017: AddedToken(\"my_MM\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20018: AddedToken(\"ne_NP\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20019: AddedToken(\"nl_XX\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20020: AddedToken(\"ro_RO\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20021: AddedToken(\"ru_RU\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20022: AddedToken(\"si_LK\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20023: AddedToken(\"tr_TR\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20024: AddedToken(\"vi_VN\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20025: AddedToken(\"zh_CN\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20026: AddedToken(\"af_ZA\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20027: AddedToken(\"az_AZ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20028: AddedToken(\"bn_IN\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20029: AddedToken(\"fa_IR\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20030: AddedToken(\"he_IL\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20031: AddedToken(\"hr_HR\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20032: AddedToken(\"id_ID\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20033: AddedToken(\"ka_GE\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20034: AddedToken(\"km_KH\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20035: AddedToken(\"mk_MK\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20036: AddedToken(\"ml_IN\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20037: AddedToken(\"mn_MN\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20038: AddedToken(\"mr_IN\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20039: AddedToken(\"pl_PL\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20040: AddedToken(\"ps_AF\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20041: AddedToken(\"pt_XX\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20042: AddedToken(\"sv_SE\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20043: AddedToken(\"sw_KE\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20044: AddedToken(\"ta_IN\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20045: AddedToken(\"te_IN\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20046: AddedToken(\"th_TH\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20047: AddedToken(\"tl_XX\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20048: AddedToken(\"uk_UA\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20049: AddedToken(\"ur_PK\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20050: AddedToken(\"xh_ZA\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20051: AddedToken(\"gl_ES\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20052: AddedToken(\"sl_SI\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20053: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/teamspace/studios/this_studio/kreol-benchmark/experiments/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6c21299dff136ec5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ubuntu/.cache/huggingface/datasets/json/default-6c21299dff136ec5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 15807.68it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 2451.85it/s]\n",
      "                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/default-6c21299dff136ec5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1331.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={'train':'/mnt/disk/yrajcoomar/kreol-benchmark/experiments/data/en-cr/en-cr_train.jsonl','test':'/mnt/disk/yrajcoomar/kreol-benchmark/experiments/data/en-cr/en-cr_test.jsonl',\n",
    "                'val':'/mnt/disk/yrajcoomar/kreol-benchmark/experiments/data/en-cr/en-cr_dev.jsonl'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'target'],\n",
       "        num_rows: 21810\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'target'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input', 'target'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c0171db8ad49c7bd79a84c739a28f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b46767022d46069472f969ae99c181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f48c8097364df7b247c81ccd8372e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples['input']\n",
    "    outputs = examples['target']\n",
    "    input_tokenized = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    output_tokenized = tokenizer(outputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    input_ids = input_tokenized[\"input_ids\"]\n",
    "    attention_mask = input_tokenized[\"attention_mask\"]\n",
    "    decoder_input_ids = output_tokenized[\"input_ids\"]\n",
    "    decoder_attention_mask = output_tokenized[\"attention_mask\"]\n",
    "    labels = decoder_input_ids.copy()\n",
    "    # Set the labels to -100 for padding tokens\n",
    "    labels = [[-100 if token == tokenizer.pad_token_id else token for token in seq] for seq in labels]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"decoder_input_ids\": decoder_input_ids, \"decoder_attention_mask\": decoder_attention_mask, \"labels\": labels}\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "val_dataset = dataset['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20004,\n",
       " 71,\n",
       " 5048,\n",
       " 30,\n",
       " 7981,\n",
       " 2829,\n",
       " 88,\n",
       " 14,\n",
       " 525,\n",
       " 85,\n",
       " 13011,\n",
       " 74,\n",
       " 30,\n",
       " 12955,\n",
       " 19974,\n",
       " 19930,\n",
       " 15218,\n",
       " 331,\n",
       " 458,\n",
       " 88,\n",
       " 19940,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['input_ids'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available() ## Current issue with cuda, will seek to resolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartConfig, MBartForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments #, Seq2SeqTrainer\n",
    "\n",
    "config = MBartConfig(vocab_size=TOKENIZER_VOCABULARY,max_position_embeddings=512)\n",
    "model = MBartForConditionalGeneration(config)\n",
    "\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./checkpoint',\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    per_gpu_eval_batch_size=4,\n",
    "    save_steps=2,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'],\n",
       "    num_rows: 21810\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATELOG:\n",
    "- 21/02/24 - Set up whole pipeline. Issue is cuda is only 9.1 on sv. so i can only use cudatoolkit 9.0 and torch==1.1\n",
    "- 22/02/24 - Got rid of a bunch of bugs in the past week, some attention mask shape issue is up\n",
    "- 27/02/24 - All issues resolved, we got it to train!!\n",
    "- 29/02/24 - We trained. We got a BLEU of 4 on 50 epochs. (Baseline is 9) Set 100 epochs to train.\n",
    "- 29/02/24 - BLEU 5 on 100 epochs. \n",
    "\n",
    "### TODO:\n",
    "- Clean repo before merging in master. Purge fairseq stuff.\n",
    "\n",
    "- Set up WandB for model eval and all that shit\n",
    "\n",
    "- Visit possible args to train model better. Ideas are vocabulary size, label_smoothing, weight decay, adam betas, dropout, attention dropout.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379375616"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "#380M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LineByLineTextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/cuda_kreol/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./all_sentences.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_concat_jsonl(files,home_dir):\n",
    "    dfs = [pd.read_json(os.path.join(home_dir,file), lines=True) for file in files]\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "def concatenate_language_pairs(language_pairs_paths,home_dir):\n",
    "    concatenated_dfs = {}\n",
    "    for key, paths in language_pairs_paths.items():\n",
    "        dfs = [read_concat_jsonl(paths,home_dir)]\n",
    "        concatenated_dfs[key] = pd.concat(dfs)\n",
    "    return concatenated_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_pairs_paths ={\n",
    "    'en_cr': ['kreol-benchmark\\experiments\\data\\en-cr\\en-cr_dev.jsonl','kreol-benchmark\\experiments\\data\\en-cr\\en-cr_train.jsonl','kreol-benchmark\\experiments\\data\\en-cr\\en-cr_test.jsonl'],\n",
    "    'cr':['kreol-benchmark\\experiments\\data\\cr\\cr_dev.jsonl','kreol-benchmark\\experiments\\data\\cr\\cr_train.jsonl','kreol-benchmark\\experiments\\data\\cr\\cr_test.jsonl']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_dict = concatenate_language_pairs(language_pairs_paths,home_dir=r'C:\\Users\\yush\\OneDrive\\Desktop\\papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_cr':                                                  input  \\\n",
       " 0    I did not come to do away with them, but to gi...   \n",
       " 1    The fact is, at the time, you had to pay the t...   \n",
       " 2    Angina can be described as a discomfort, heavi...   \n",
       " 3             The boy said he would, but he didn't go.   \n",
       " 4     Was it God in heaven or merely some human being?   \n",
       " ..                                                 ...   \n",
       " 995  Any kingdom where people fight each other will...   \n",
       " 996  And I am not good enough even to stoop down an...   \n",
       " 997  Who among you, if your son asks for bread, you...   \n",
       " 998  If that person listens, you have won back a fo...   \n",
       " 999  Then he pointed to his disciples and said, the...   \n",
       " \n",
       "                                                 target  \n",
       " 0    Mo pa finn vini pou aboli me pou donn zot zot ...  \n",
       " 1    Anverite sa lepok la pou al lekol ti ena enn f...  \n",
       " 2    Nou capav dekrir anzinn couma enn sensasion in...  \n",
       " 3    Garson-la reponn wi papa, li pou ale me li pa ...  \n",
       " 4    Eski sa ti sorti depi dan lesiel ouswa dimoun ...  \n",
       " ..                                                 ...  \n",
       " 995  Enn rwayom ki divize kont limem, li enn rwayom...  \n",
       " 996  E mo pa ase bon, pou mo kourbe devan li, pou l...  \n",
       " 997  Kisannla parmi zot, ki si so garson dimann dip...  \n",
       " 998  Si sa dimounn la ekoute, to pou finn regagn li...  \n",
       " 999  Lerla li montre zot so bann disip e li dir, al...  \n",
       " \n",
       " [23310 rows x 2 columns],\n",
       " 'cr':                                                    input target\n",
       " 0                                                              \n",
       " 0                                                              \n",
       " 1      Trak-la pir ki so diskur divan Mauritius Telecom.       \n",
       " 2      MMM, dan so komansman, antan ki enn gran muvma...       \n",
       " 3      Onivo LALIT, nu finn gayn gran difikilte kone ...       \n",
       " ...                                                  ...    ...\n",
       " 45360            Me, anu get MSM ek MMM zot rol dan DWC.       \n",
       " 45361  Marx ti temwayn sa kapasite travayer Angle, pu...       \n",
       " 45362  Eski klas travayer an-mezir pu dikte ki pu ena...       \n",
       " 45363                                     Dimoun ti per.       \n",
       " 0                                                              \n",
       " \n",
       " [45366 rows x 2 columns]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
