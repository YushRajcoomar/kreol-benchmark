{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build the model off of HuggingFace libraries instead of fairseq scripts. It is easier to debug and understand what you're doing when building the model.\n",
    "We are starting from absolute scratch. We need to train a sentencepiece tokenizer on joint corpus. That's kinda done in the previous notebook.\n",
    "\n",
    "However I never got a hf model to train with the tokenizer. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers.implementations import SentencePieceBPETokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Model from hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_SIZE = 256  # Batch-size to train the tokenizer on\n",
    "TOKENIZER_VOCABULARY = 25000  # Total number of unique subwords the tokenizer can have\n",
    "\n",
    "BLOCK_SIZE = 128  # Maximum number of tokens in an input sample\n",
    "NSP_PROB = 0.50  # Probability that the next sentence is the actual next sentence in NSP\n",
    "SHORT_SEQ_PROB = 0.1  # Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding\n",
    "\n",
    "MLM_PROB = 0.2  # Probability with which tokens are masked in MLM\n",
    "\n",
    "TRAIN_BATCH_SIZE = 2  # Batch-size for pretraining the model on\n",
    "MAX_EPOCHS = 1  # Maximum number of epochs to train the model for\n",
    "LEARNING_RATE = 1e-4  # Learning rate for training the model\n",
    "\n",
    "MODEL_CHECKPOINT = \"mbart-large-50\"  # Name of pretrained model from ðŸ¤— Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding all the data i have in 1 large text file to train my tokenizer\n",
    "\n",
    "# paths = [str(x) for x in Path(\"../experiments/data/\").glob(\"**/*.jsonl\")]\n",
    "# paths_df = [pd.read_json(x,lines=True) for x in paths]\n",
    "\n",
    "# all_str = ''\n",
    "# for i in paths_df:\n",
    "#     add_str = ''.join(i['input'].values + i['target'].values)\n",
    "#     all_str += add_str\n",
    "\n",
    "# sentences = all_str.split(\". \")\n",
    "\n",
    "# with open(\"all_sentences.txt\", \"w\") as file:\n",
    "#     for sentence in sentences:\n",
    "#         file.write(sentence + \".\\n\")\n",
    "\n",
    "# !spm_train --input=\"/mnt/disk/yrajcoomar/kreol-benchmark/pipelines/all_sentences.txt\" --model_prefix=kreol --vocab_size=20000 --model_type=bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train tokenizer\n",
    "\n",
    "# def batch_iterator():\n",
    "#     for i in range(0, len(all_str), TOKENIZER_BATCH_SIZE):\n",
    "#         yield all_str[i : i + TOKENIZER_BATCH_SIZE]\n",
    "\n",
    "# tokenizer = SentencePieceBPETokenizer()\n",
    "# tokenizer.train_from_iterator(batch_iterator(), vocab_size=TOKENIZER_BATCH_SIZE, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "# ])\n",
    "\n",
    "\n",
    "# !mkdir tok\n",
    "# tokenizer.save_model(\"./tok/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## idrk whats going on\n",
    "\n",
    "# from tokenizers.implementations import SentencePieceBPETokenizer\n",
    "# from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "# tokenizer = SentencePieceBPETokenizer(\n",
    "#     \"tokenizer/vocab.json\",\n",
    "#     \"tokenizer/merges.txt\",\n",
    "# )\n",
    "# tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#     (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#     (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "# )\n",
    "# tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/cuda_kreol/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBart50Tokenizer\n",
    "\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"./tok/\",max_len=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0cd2e6f29b60aba8\n",
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/default-0cd2e6f29b60aba8/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1278.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={'train':'/mnt/disk/yrajcoomar/kreol-benchmark/experiments/data/en-cr/en-cr_train.jsonl','test':'/mnt/disk/yrajcoomar/kreol-benchmark/experiments/data/en-cr/en-cr_test.jsonl',\n",
    "                'val':'/mnt/disk/yrajcoomar/kreol-benchmark/experiments/data/en-cr/en-cr_dev.jsonl'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x7f1202520d40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 21/22 [00:04<00:00,  4.76ba/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples['input']\n",
    "    outputs = examples['target']\n",
    "    inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    outputs = tokenizer(outputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"input_ids\"] = [[tokenizer.bos_token_id] + ids + [tokenizer.eos_token_id] for ids in inputs[\"input_ids\"]]\n",
    "    outputs[\"input_ids\"] = [[tokenizer.bos_token_id] + ids + [tokenizer.eos_token_id] for ids in outputs[\"input_ids\"]]\n",
    "    return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"], \"decoder_input_ids\": outputs[\"input_ids\"], \"decoder_attention_mask\": outputs[\"attention_mask\"]}\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "val_dataset = dataset['val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available() ## Current issue with cuda, will seek to resolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartConfig, MBartForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments #, Seq2SeqTrainer\n",
    "\n",
    "config = MBartConfig(vocab_size=TOKENIZER_VOCABULARY,max_position_embeddings=512)\n",
    "model = MBartForConditionalGeneration(config)\n",
    "\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./checkpoint',\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    per_gpu_eval_batch_size=4,\n",
    "    save_steps=2,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPDATELOG:\n",
    "# 21/02/24 - Set up whole pipeline. Issue is cuda is only 9.1 on sv. so i can only use cudatoolkit 9.0 and torch==1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379375616"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "#380M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LineByLineTextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/cuda_kreol/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./all_sentences.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_concat_jsonl(files,home_dir):\n",
    "    dfs = [pd.read_json(os.path.join(home_dir,file), lines=True) for file in files]\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "def concatenate_language_pairs(language_pairs_paths,home_dir):\n",
    "    concatenated_dfs = {}\n",
    "    for key, paths in language_pairs_paths.items():\n",
    "        dfs = [read_concat_jsonl(paths,home_dir)]\n",
    "        concatenated_dfs[key] = pd.concat(dfs)\n",
    "    return concatenated_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_pairs_paths ={\n",
    "    'en_cr': ['kreol-benchmark\\experiments\\data\\en-cr\\en-cr_dev.jsonl','kreol-benchmark\\experiments\\data\\en-cr\\en-cr_train.jsonl','kreol-benchmark\\experiments\\data\\en-cr\\en-cr_test.jsonl'],\n",
    "    'cr':['kreol-benchmark\\experiments\\data\\cr\\cr_dev.jsonl','kreol-benchmark\\experiments\\data\\cr\\cr_train.jsonl','kreol-benchmark\\experiments\\data\\cr\\cr_test.jsonl']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_dict = concatenate_language_pairs(language_pairs_paths,home_dir=r'C:\\Users\\yush\\OneDrive\\Desktop\\papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_cr':                                                  input  \\\n",
       " 0    I did not come to do away with them, but to gi...   \n",
       " 1    The fact is, at the time, you had to pay the t...   \n",
       " 2    Angina can be described as a discomfort, heavi...   \n",
       " 3             The boy said he would, but he didn't go.   \n",
       " 4     Was it God in heaven or merely some human being?   \n",
       " ..                                                 ...   \n",
       " 995  Any kingdom where people fight each other will...   \n",
       " 996  And I am not good enough even to stoop down an...   \n",
       " 997  Who among you, if your son asks for bread, you...   \n",
       " 998  If that person listens, you have won back a fo...   \n",
       " 999  Then he pointed to his disciples and said, the...   \n",
       " \n",
       "                                                 target  \n",
       " 0    Mo pa finn vini pou aboli me pou donn zot zot ...  \n",
       " 1    Anverite sa lepok la pou al lekol ti ena enn f...  \n",
       " 2    Nou capav dekrir anzinn couma enn sensasion in...  \n",
       " 3    Garson-la reponn wi papa, li pou ale me li pa ...  \n",
       " 4    Eski sa ti sorti depi dan lesiel ouswa dimoun ...  \n",
       " ..                                                 ...  \n",
       " 995  Enn rwayom ki divize kont limem, li enn rwayom...  \n",
       " 996  E mo pa ase bon, pou mo kourbe devan li, pou l...  \n",
       " 997  Kisannla parmi zot, ki si so garson dimann dip...  \n",
       " 998  Si sa dimounn la ekoute, to pou finn regagn li...  \n",
       " 999  Lerla li montre zot so bann disip e li dir, al...  \n",
       " \n",
       " [23310 rows x 2 columns],\n",
       " 'cr':                                                    input target\n",
       " 0                                                              \n",
       " 0                                                              \n",
       " 1      Trak-la pir ki so diskur divan Mauritius Telecom.       \n",
       " 2      MMM, dan so komansman, antan ki enn gran muvma...       \n",
       " 3      Onivo LALIT, nu finn gayn gran difikilte kone ...       \n",
       " ...                                                  ...    ...\n",
       " 45360            Me, anu get MSM ek MMM zot rol dan DWC.       \n",
       " 45361  Marx ti temwayn sa kapasite travayer Angle, pu...       \n",
       " 45362  Eski klas travayer an-mezir pu dikte ki pu ena...       \n",
       " 45363                                     Dimoun ti per.       \n",
       " 0                                                              \n",
       " \n",
       " [45366 rows x 2 columns]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
