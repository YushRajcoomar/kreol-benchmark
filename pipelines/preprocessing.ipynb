{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build the model off of HuggingFace libraries instead of fairseq scripts. It is easier to debug and understand what you're doing when building the model.\n",
    "We are starting from absolute scratch. We need to train a sentencepiece tokenizer on joint corpus. That's kinda done in the previous notebook.\n",
    "\n",
    "However I never got a hf model to train with the tokenizer. Let's try that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Model from hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_SIZE = 256  # Batch-size to train the tokenizer on\n",
    "TOKENIZER_VOCABULARY = 25000  # Total number of unique subwords the tokenizer can have\n",
    "\n",
    "BLOCK_SIZE = 128  # Maximum number of tokens in an input sample\n",
    "NSP_PROB = 0.50  # Probability that the next sentence is the actual next sentence in NSP\n",
    "SHORT_SEQ_PROB = 0.1  # Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding\n",
    "\n",
    "MLM_PROB = 0.2  # Probability with which tokens are masked in MLM\n",
    "\n",
    "TRAIN_BATCH_SIZE = 2  # Batch-size for pretraining the model on\n",
    "MAX_EPOCHS = 1  # Maximum number of epochs to train the model for\n",
    "LEARNING_RATE = 1e-4  # Learning rate for training the model\n",
    "\n",
    "MODEL_CHECKPOINT = \"mbart-large-50\"  # Name of pretrained model from ðŸ¤— Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29840\\1760743779.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_CHECKPOINT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_concat_jsonl(files,home_dir):\n",
    "    dfs = [pd.read_json(os.path.join(home_dir,file), lines=True) for file in files]\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "def concatenate_language_pairs(language_pairs_paths,home_dir):\n",
    "    concatenated_dfs = {}\n",
    "    for key, paths in language_pairs_paths.items():\n",
    "        dfs = [read_concat_jsonl(paths,home_dir)]\n",
    "        concatenated_dfs[key] = pd.concat(dfs)\n",
    "    return concatenated_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_pairs_paths ={\n",
    "    'en_cr': ['kreol-benchmark\\experiments\\data\\en-cr\\en-cr_dev.jsonl','kreol-benchmark\\experiments\\data\\en-cr\\en-cr_train.jsonl','kreol-benchmark\\experiments\\data\\en-cr\\en-cr_test.jsonl'],\n",
    "    'cr':['kreol-benchmark\\experiments\\data\\cr\\cr_dev.jsonl','kreol-benchmark\\experiments\\data\\cr\\cr_train.jsonl','kreol-benchmark\\experiments\\data\\cr\\cr_test.jsonl']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_dict = concatenate_language_pairs(language_pairs_paths,home_dir=r'C:\\Users\\yush\\OneDrive\\Desktop\\papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_cr':                                                  input  \\\n",
       " 0    I did not come to do away with them, but to gi...   \n",
       " 1    The fact is, at the time, you had to pay the t...   \n",
       " 2    Angina can be described as a discomfort, heavi...   \n",
       " 3             The boy said he would, but he didn't go.   \n",
       " 4     Was it God in heaven or merely some human being?   \n",
       " ..                                                 ...   \n",
       " 995  Any kingdom where people fight each other will...   \n",
       " 996  And I am not good enough even to stoop down an...   \n",
       " 997  Who among you, if your son asks for bread, you...   \n",
       " 998  If that person listens, you have won back a fo...   \n",
       " 999  Then he pointed to his disciples and said, the...   \n",
       " \n",
       "                                                 target  \n",
       " 0    Mo pa finn vini pou aboli me pou donn zot zot ...  \n",
       " 1    Anverite sa lepok la pou al lekol ti ena enn f...  \n",
       " 2    Nou capav dekrir anzinn couma enn sensasion in...  \n",
       " 3    Garson-la reponn wi papa, li pou ale me li pa ...  \n",
       " 4    Eski sa ti sorti depi dan lesiel ouswa dimoun ...  \n",
       " ..                                                 ...  \n",
       " 995  Enn rwayom ki divize kont limem, li enn rwayom...  \n",
       " 996  E mo pa ase bon, pou mo kourbe devan li, pou l...  \n",
       " 997  Kisannla parmi zot, ki si so garson dimann dip...  \n",
       " 998  Si sa dimounn la ekoute, to pou finn regagn li...  \n",
       " 999  Lerla li montre zot so bann disip e li dir, al...  \n",
       " \n",
       " [23310 rows x 2 columns],\n",
       " 'cr':                                                    input target\n",
       " 0                                                              \n",
       " 0                                                              \n",
       " 1      Trak-la pir ki so diskur divan Mauritius Telecom.       \n",
       " 2      MMM, dan so komansman, antan ki enn gran muvma...       \n",
       " 3      Onivo LALIT, nu finn gayn gran difikilte kone ...       \n",
       " ...                                                  ...    ...\n",
       " 45360            Me, anu get MSM ek MMM zot rol dan DWC.       \n",
       " 45361  Marx ti temwayn sa kapasite travayer Angle, pu...       \n",
       " 45362  Eski klas travayer an-mezir pu dikte ki pu ena...       \n",
       " 45363                                     Dimoun ti per.       \n",
       " 0                                                              \n",
       " \n",
       " [45366 rows x 2 columns]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
